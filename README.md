# Distillation Techniques for Pseudo-rehearsal Based Incremental Learning
This repository contains code for the paper [Distillation Techniques for Pseudo-rehearsal Based Incremental Learning](https://arxiv.org/abs/1807.02799). It also contains the code to reproduce the results of [iCarl](https://arxiv.org/abs/1611.07725) paper. The implementation is done using Pytorch.

## Requirements
The requirements.txt file contains all the requirements along with their specific versions. These requirements can be installed by using `pip install -r requirements.txt`

## Usage
TODO

## Results
TODO

## Paper citation
If you used this code for your experiments or found it helpful, consider citing the following paper:
```
@article{2018arXiv180702799S,
   author = {{Shah}, H. and {Javed}, K. and {Shafait}, F.},
    title = "{Distillation Techniques for Pseudo-rehearsal Based Incremental Learning}",
  journal = {ArXiv e-prints},
     year = 2018,
}
```
